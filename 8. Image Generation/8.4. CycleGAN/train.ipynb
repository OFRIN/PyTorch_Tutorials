{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1KQYSQvsJmxjtC2OUXZzP33xeRoSuGwPB","authorship_tag":"ABX9TyO78pHH6fGguKjgeqwjTvvJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4vnWeENwa4b","executionInfo":{"status":"ok","timestamp":1605777649409,"user_tz":-540,"elapsed":882,"user":{"displayName":"조상현","photoUrl":"","userId":"10768189644336602050"}},"outputId":"41661313-4a1b-4346-d378-ba1ad8c1cd44"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qQRM5e6yoti","executionInfo":{"status":"ok","timestamp":1605777502920,"user_tz":-540,"elapsed":1063,"user":{"displayName":"조상현","photoUrl":"","userId":"10768189644336602050"}},"outputId":"29c2d6c0-4949-477f-c9fa-3a67f35e27f0"},"source":["%cd /content/gdrive/My\\ Drive\n","%cd PyTorch_CycleGAN_Example/"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive\n","/content/gdrive/My Drive/PyTorch_CycleGAN_Example\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PAFdEBS6zEZA","executionInfo":{"status":"ok","timestamp":1605777945235,"user_tz":-540,"elapsed":612,"user":{"displayName":"조상현","photoUrl":"","userId":"10768189644336602050"}}},"source":["import os\n","import cv2\n","import torch\n","import itertools\n","\n","from PIL import Image\n","from torchvision import transforms\n","\n","from core.dataset import ImageDataset\n","from core.networks import Generator, Discriminator\n","from core.utils import *\n","\n","from util.time_utils import Timer\n","from util.utils import *\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uGYoy2jJzIeB","executionInfo":{"status":"ok","timestamp":1605777662941,"user_tz":-540,"elapsed":950,"user":{"displayName":"조상현","photoUrl":"","userId":"10768189644336602050"}},"outputId":"8ebfa279-4989-418d-df5a-83ab012f22d8"},"source":["torch.__version__"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.7.0+cu101'"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"IUGbUWdm0bHX","executionInfo":{"status":"ok","timestamp":1605777948276,"user_tz":-540,"elapsed":833,"user":{"displayName":"조상현","photoUrl":"","userId":"10768189644336602050"}}},"source":["# 1. Define\n","root_dir = './datasets/horse2zebra/'\n","\n","# /content/gdrive/My Drive/PyTorch_CycleGAN_Example/model/\n","model_dir = './model/'\n","if not os.path.isdir(model_dir):\n","    os.makedirs(model_dir)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"coj6MEFn0ifv","executionInfo":{"status":"ok","timestamp":1605777953712,"user_tz":-540,"elapsed":1017,"user":{"displayName":"조상현","photoUrl":"","userId":"10768189644336602050"}}},"source":["learning_rate = 0.0002\n","batch_size = 2\n","image_size = 256\n","A_channels = 3\n","B_channels = 3\n","max_epoch = 200"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGZQeQEe0oH6","executionInfo":{"status":"ok","timestamp":1605777955394,"user_tz":-540,"elapsed":991,"user":{"displayName":"조상현","photoUrl":"","userId":"10768189644336602050"}},"outputId":"9e0dc3f8-4e25-4ec5-bd30-2597af14c31a"},"source":["# 2. Dataset\n","transform = transforms.Compose(\n","    [\n","        transforms.Resize(int(image_size * 1.12), Image.BICUBIC),\n","        transforms.RandomCrop(image_size),\n","        transforms.RandomHorizontalFlip(),\n","\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ]\n",")\n","\n","A_dataset = ImageDataset(root_dir, transform=transform, mode='trainA')\n","B_dataset = ImageDataset(root_dir, transform=transform, mode='trainB')\n","\n","A_loader = torch.utils.data.DataLoader(A_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","B_loader = torch.utils.data.DataLoader(B_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","\n","# 3. Train\n","G_A2B = Generator(A_channels, B_channels, 9).cuda()\n","G_B2A = Generator(B_channels, A_channels, 9).cuda()\n","D_A = Discriminator(A_channels).cuda()\n","D_B = Discriminator(B_channels).cuda()\n","\n","G_A2B.apply(weights_init_normal)\n","G_B2A.apply(weights_init_normal)\n","D_A.apply(weights_init_normal)\n","D_B.apply(weights_init_normal)\n","\n","# 4. Loss\n","GAN_loss_fn = torch.nn.MSELoss().cuda()\n","cycle_loss_fn = torch.nn.L1Loss().cuda()\n","identity_loss_fn = torch.nn.L1Loss().cuda()\n","\n","# 5. Optimizer\n","optimizer_G = torch.optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n","optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","\n","lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(max_epoch, 0, max_epoch // 2).step)\n","lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(max_epoch, 0, max_epoch // 2).step)\n","lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(max_epoch, 0, max_epoch // 2).step)\n","\n","# 6. Training\n","# input_A = torch.Tensor(batch_size, A_channels, image_size, image_size)\n","# input_B = torch.Tensor(batch_size, B_channels, image_size, image_size)\n","real_labels = torch.autograd.Variable(torch.Tensor(batch_size, 1).fill_(1.0), requires_grad=False).cuda()\n","fake_labels = torch.autograd.Variable(torch.Tensor(batch_size, 1).fill_(0.0), requires_grad=False).cuda()\n","\n","train_timer = Timer()\n","train_avg = Average_Meter([\n","    'identity_A_loss', 'identity_B_loss', \n","    'cycle_A_loss', 'cycle_B_loss', \n","    'G_A2B_loss', 'G_B2A_loss', \n","    'G_loss',\n","    'D_A_loss', 'D_B_loss',\n","])"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/PyTorch_CycleGAN_Example/core/utils.py:8: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n","  torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUn8IdebxBRB","outputId":"fc826595-fa38-45c5-aac7-bf547fe008f1"},"source":["torch.save(G_A2B.state_dict(), model_dir + 'G_A2B.pth')\n","torch.save(G_B2A.state_dict(), model_dir + 'G_B2A.pth')\n","torch.save(D_A.state_dict(), model_dir + 'D_A.pth')\n","torch.save(D_B.state_dict(), model_dir + 'D_B.pth')\n","print('ex) save files')\n","\n","for epoch in range(1, max_epoch + 1):\n","\n","    train_avg.clear()\n","    train_timer.tik()\n","\n","    for real_A_images, real_B_images in zip(A_loader, B_loader):\n","        real_A_images = real_A_images.cuda()\n","        real_B_images = real_B_images.cuda()\n","        \n","        # 1. Identity Loss\n","        same_B_images = G_A2B(real_B_images)\n","        same_A_images = G_B2A(real_A_images)\n","        \n","        identity_A_loss = identity_loss_fn(same_A_images, real_A_images) * 5.\n","        identity_B_loss = identity_loss_fn(same_B_images, real_B_images) * 5.\n","\n","        # 2. GAN Loss\n","        fake_B_images = G_A2B(real_A_images)\n","        fake_B_logits = D_B(fake_B_images)\n","\n","        fake_A_images = G_B2A(real_B_images)\n","        fake_A_logits = D_A(fake_A_images)\n","\n","        G_A2B_loss = GAN_loss_fn(fake_B_logits, real_labels)\n","        G_B2A_loss = GAN_loss_fn(fake_A_logits, real_labels)\n","        \n","        # 3. Cycle Consistency Loss\n","        reconstruction_A_images = G_B2A(fake_B_images)\n","        reconstruction_B_images = G_A2B(fake_A_images)\n","\n","        cycle_A_loss = cycle_loss_fn(reconstruction_A_images, real_A_images) * 10.\n","        cycle_B_loss = cycle_loss_fn(reconstruction_B_images, real_B_images) * 10.\n","\n","        G_loss = identity_A_loss + identity_B_loss + G_A2B_loss + G_B2A_loss + cycle_A_loss + cycle_B_loss\n","\n","        optimizer_G.zero_grad()\n","        G_loss.backward()\n","        optimizer_G.step()\n","\n","        # Discriminator Losses\n","        real_A_logits = D_A(real_A_images)\n","        fake_A_logits = D_A(fake_A_images.detach())\n","\n","        D_A_loss = (GAN_loss_fn(fake_A_logits, fake_labels) + GAN_loss_fn(real_A_logits, real_labels)) / 2.\n","\n","        optimizer_D_A.zero_grad()\n","        D_A_loss.backward(retain_graph=True)\n","        optimizer_D_A.step()\n","\n","        real_B_logits = D_B(real_B_images)\n","        fake_B_logits = D_B(fake_B_images.detach())\n","\n","        D_B_loss = (GAN_loss_fn(fake_B_logits, fake_labels) + GAN_loss_fn(real_B_logits, real_labels)) / 2.\n","\n","        optimizer_D_B.zero_grad()\n","        D_B_loss.backward(retain_graph=True)\n","        optimizer_D_B.step()\n","\n","        train_avg.add({\n","            'identity_A_loss' : identity_A_loss.item(), 'identity_B_loss' : identity_B_loss.item(), \n","            'cycle_A_loss' : cycle_A_loss.item(), 'cycle_B_loss' : cycle_B_loss.item(), \n","            'G_A2B_loss' : G_A2B_loss.item(), 'G_B2A_loss' : G_B2A_loss.item(), \n","            'G_loss' : G_loss.item(),\n","            'D_A_loss' : D_A_loss.item(), 'D_B_loss' : D_B_loss.item(),\n","        })\n","\n","    data = [epoch] + train_avg.get(clear=True) + [train_timer.tok()]\n","    print('[i] epoch={}, identity_A_loss={:.4f}, identity_B_loss={:.4f}, cycle_A_loss={:.4f}, cycle_B_loss={:.4f}, G_A2B_loss={:.4f}, G_B2A_loss={:.4f}, G_loss={:.4f}, D_A_loss={:.4f}, D_B_loss={:.4f}, time={}sec'.format(*data))\n","\n","    lr_scheduler_G.step()\n","    lr_scheduler_D_A.step()\n","    lr_scheduler_D_B.step()\n","\n","    torch.save(G_A2B.state_dict(), model_dir + 'G_A2B.pth')\n","    torch.save(G_B2A.state_dict(), model_dir + 'G_B2A.pth')\n","    torch.save(D_A.state_dict(), model_dir + 'D_A.pth')\n","    torch.save(D_B.state_dict(), model_dir + 'D_B.pth')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ex) save files\n","[i] epoch=1, identity_A_loss=1.1327, identity_B_loss=1.2534, cycle_A_loss=2.4696, cycle_B_loss=2.7098, G_A2B_loss=0.4484, G_B2A_loss=0.4059, G_loss=8.4198, D_A_loss=0.2519, D_B_loss=0.2343, time=351sec\n"],"name":"stdout"}]}]}